# Awesome-Foundation-Models
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of foundation models for vision and language tasks. Papers without code are not included.

## Papers

### Survey

* [2023.03] [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT](https://arxiv.org/pdf/2302.09419.pdf)
* [2022.07] [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf)

### 2023

* [arXiv 2023.03] [Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models.](https://arxiv.org/pdf/2303.04671.pdf) (from Microsoft Research Asia)
* [CVPR] [UNINEXT: Universal Instance Perception as Object Discovery and Retrieval.](https://arxiv.org/pdf/2303.06674.pdf) (from ByteDance)
* [CVPR] [InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions.](https://arxiv.org/pdf/2211.05778.pdf) (from Shanghai AI Laboratory)

### 2022

* [CVPR] [Video Swin Transformer.](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Video_Swin_Transformer_CVPR_2022_paper.pdf) (from Microsoft Research Asia)
* [CVPR] [FLAVA: A Foundational Language And Vision Alignment Model.](https://openaccess.thecvf.com/content/CVPR2022/papers/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.pdf) (from Facebook AI Research)
* [Nature Comm.] [Towards artificial general intelligence via a multimodal foundation model.](https://www.nature.com/articles/s41467-022-30761-2) (from Renmin University of China)

### 2021

* [ICCV] [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf) (from Microsoft Research Asia)
* [ICLR] [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.](https://arxiv.org/pdf/2010.11929.pdf) (from Google)

### 2017

* [NeurIPS] [Attention Is All You Need.](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (from Google)
